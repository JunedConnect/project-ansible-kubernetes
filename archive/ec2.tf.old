resource "aws_instance" "bastion_host" {

  ami             = "ami-0a0ff88d0f3f85a14" # Ubuntu 22.04 LTS
  instance_type   = "t3.micro"
  subnet_id       = module.vpc.public_subnet_1_id
  security_groups = [aws_security_group.bastionhost.id]
  key_name        = aws_key_pair.this.key_name
  tags = {
    name    = "bastion-host"
    role    = "bastion-host"
    project = "ansible-kubernetes"
  }

  # user_data= file("userdata-cp.sh")

  lifecycle {
    ignore_changes = [
      security_groups
    ]
  }
}

resource "aws_instance" "control_plane" {

  ami             = "ami-0a0ff88d0f3f85a14" # Ubuntu 22.04 LTS
  instance_type   = "t3.medium"
  subnet_id       = module.vpc.private_subnet_1_id # make sure to move to private subnet in the end
  security_groups = [aws_security_group.controlplane.id]
  key_name        = aws_key_pair.this.key_name
  tags = {
    name    = "control-plane"
    role    = "control-plane"
    project = "ansible-kubernetes"
  }

  # user_data= file("userdata-cp.sh")

  lifecycle {
    ignore_changes = [
      security_groups
    ]
  }
}

resource "aws_instance" "worker_node_1" {

  ami             = "ami-0a0ff88d0f3f85a14" # Ubuntu 22.04 LTS
  instance_type   = "t3.micro"
  subnet_id       = module.vpc.private_subnet_1_id
  security_groups = [aws_security_group.workernode.id]
  key_name        = aws_key_pair.this.key_name
  tags = {
    name    = "worker-node-1"
    role    = "worker-node"
    project = "ansible-kubernetes"
  }

  # user_data= file("userdata-wn1.sh")

  lifecycle {
    ignore_changes = [
      security_groups
    ]
  }
}

resource "aws_instance" "worker_node_2" {

  ami             = "ami-0a0ff88d0f3f85a14" # Ubuntu 22.04 LTS
  instance_type   = "t3.micro"
  subnet_id       = module.vpc.private_subnet_2_id
  security_groups = [aws_security_group.workernode.id]
  key_name        = aws_key_pair.this.key_name
  tags = {
    name    = "worker-node-2"
    role    = "worker-node"
    project = "ansible-kubernetes"
  }

  # user_data= file("userdata-wn2.sh")

  lifecycle {
    ignore_changes = [
      security_groups
    ]
  }
}

resource "aws_key_pair" "this" {
  key_name   = "playground-key"
  public_key = file("~/.ssh/playground.pub")
}


resource "aws_security_group" "bastionhost" {
  name        = "bastionhost"
  description = "Bsation Host SG"
  vpc_id      = module.vpc.vpc_id
}

resource "aws_security_group" "controlplane" {
  name        = "controlplane"
  description = "Kubernetes control plane SG"
  vpc_id      = module.vpc.vpc_id
}

resource "aws_security_group" "workernode" {
  name        = "workernode"
  description = "Kubernetes worker node SG"
  vpc_id      = module.vpc.vpc_id
}

# Bastion host SSH

resource "aws_vpc_security_group_ingress_rule" "bh_ssh" {
  security_group_id = aws_security_group.bastionhost.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 22
  to_port           = 22
  ip_protocol       = "tcp"
  description       = "SSH access to control plane"
}

# Control plane SSH
resource "aws_vpc_security_group_ingress_rule" "cp_ssh" {
  security_group_id = aws_security_group.controlplane.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 22
  to_port           = 22
  ip_protocol       = "tcp"
  description       = "SSH access to control plane"
}

# Kubernetes API
resource "aws_vpc_security_group_ingress_rule" "cp_api" {
  security_group_id = aws_security_group.controlplane.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 6443
  to_port           = 6443
  ip_protocol       = "tcp"
  description       = "Kubernetes API server"
}

# etcd from workers
resource "aws_vpc_security_group_ingress_rule" "cp_etcd_from_workers" {
  security_group_id            = aws_security_group.controlplane.id
  referenced_security_group_id = aws_security_group.workernode.id
  from_port                    = 2379
  to_port                      = 2380
  ip_protocol                  = "tcp"
  description                  = "etcd access from worker nodes"
}

# VXLAN overlay
resource "aws_vpc_security_group_ingress_rule" "cp_vxlan_self" {
  security_group_id            = aws_security_group.controlplane.id
  referenced_security_group_id = aws_security_group.controlplane.id
  from_port                    = 8472
  to_port                      = 8472
  ip_protocol                  = "udp"
  description                  = "VXLAN overlay from self"
}

resource "aws_vpc_security_group_ingress_rule" "cp_vxlan_from_workers" {
  security_group_id            = aws_security_group.controlplane.id
  referenced_security_group_id = aws_security_group.workernode.id
  from_port                    = 8472
  to_port                      = 8472
  ip_protocol                  = "udp"
  description                  = "VXLAN overlay from worker nodes"
}

# Cilium health TCP
resource "aws_vpc_security_group_ingress_rule" "cp_health_tcp_self" {
  security_group_id            = aws_security_group.controlplane.id
  referenced_security_group_id = aws_security_group.controlplane.id
  from_port                    = 4240
  to_port                      = 4240
  ip_protocol                  = "tcp"
  description                  = "Cilium health TCP from self"
}

resource "aws_vpc_security_group_ingress_rule" "cp_health_tcp_from_workers" {
  security_group_id            = aws_security_group.controlplane.id
  referenced_security_group_id = aws_security_group.workernode.id
  from_port                    = 4240
  to_port                      = 4240
  ip_protocol                  = "tcp"
  description                  = "Cilium health TCP from worker nodes"
}

# Kubelet API
resource "aws_vpc_security_group_ingress_rule" "cp_kubelet" {
  security_group_id            = aws_security_group.controlplane.id
  referenced_security_group_id = aws_security_group.controlplane.id
  from_port                    = 10250
  to_port                      = 10250
  ip_protocol                  = "tcp"
  description                  = "Kubelet API self/control plane"
}

# kube-scheduler
resource "aws_vpc_security_group_ingress_rule" "cp_scheduler" {
  security_group_id            = aws_security_group.controlplane.id
  referenced_security_group_id = aws_security_group.controlplane.id
  from_port                    = 10259
  to_port                      = 10259
  ip_protocol                  = "tcp"
  description                  = "kube-scheduler self-access"
}

# kube-controller-manager
resource "aws_vpc_security_group_ingress_rule" "cp_controller_manager" {
  security_group_id            = aws_security_group.controlplane.id
  referenced_security_group_id = aws_security_group.controlplane.id
  from_port                    = 10257
  to_port                      = 10257
  ip_protocol                  = "tcp"
  description                  = "kube-controller-manager self-access"
}

# Worker node SSH
resource "aws_vpc_security_group_ingress_rule" "wn_ssh" {
  security_group_id = aws_security_group.workernode.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 22
  to_port           = 22
  ip_protocol       = "tcp"
  description       = "SSH access to worker nodes"
}


# Kubelet API from control plane
resource "aws_vpc_security_group_ingress_rule" "wn_kubelet" {
  security_group_id            = aws_security_group.workernode.id
  referenced_security_group_id = aws_security_group.controlplane.id
  from_port                    = 10250
  to_port                      = 10250
  ip_protocol                  = "tcp"
  description                  = "Kubelet API from control plane"
}

# kube-proxy
resource "aws_vpc_security_group_ingress_rule" "wn_kube_proxy" {
  security_group_id            = aws_security_group.workernode.id
  referenced_security_group_id = aws_security_group.workernode.id
  from_port                    = 10256
  to_port                      = 10256
  ip_protocol                  = "tcp"
  description                  = "kube-proxy self access / metrics"
}

# NodePort TCP
resource "aws_vpc_security_group_ingress_rule" "wn_nodeport_tcp" {
  security_group_id = aws_security_group.workernode.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 30000
  to_port           = 32767
  ip_protocol       = "tcp"
  description       = "NodePort TCP services"
}

# NodePort UDP
resource "aws_vpc_security_group_ingress_rule" "wn_nodeport_udp" {
  security_group_id = aws_security_group.workernode.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 30000
  to_port           = 32767
  ip_protocol       = "udp"
  description       = "NodePort UDP services"
}

# VXLAN overlay
resource "aws_vpc_security_group_ingress_rule" "wn_vxlan_from_masters" {
  security_group_id            = aws_security_group.workernode.id
  referenced_security_group_id = aws_security_group.controlplane.id
  from_port                    = 8472
  to_port                      = 8472
  ip_protocol                  = "udp"
  description                  = "Cilium VXLAN from control plane"
}

resource "aws_vpc_security_group_ingress_rule" "wn_vxlan_self" {
  security_group_id            = aws_security_group.workernode.id
  referenced_security_group_id = aws_security_group.workernode.id
  from_port                    = 8472
  to_port                      = 8472
  ip_protocol                  = "udp"
  description                  = "Cilium VXLAN overlay self"
}

# Cilium health TCP
resource "aws_vpc_security_group_ingress_rule" "wn_health_tcp_from_masters" {
  security_group_id            = aws_security_group.workernode.id
  referenced_security_group_id = aws_security_group.controlplane.id
  from_port                    = 4240
  to_port                      = 4240
  ip_protocol                  = "tcp"
  description                  = "Cilium health TCP from master"
}

resource "aws_vpc_security_group_ingress_rule" "wn_health_tcp_self" {
  security_group_id            = aws_security_group.workernode.id
  referenced_security_group_id = aws_security_group.workernode.id
  from_port                    = 4240
  to_port                      = 4240
  ip_protocol                  = "tcp"
  description                  = "Cilium health TCP self"
}

# ingress icmp

resource "aws_vpc_security_group_ingress_rule" "wn_icmp" {
  security_group_id = aws_security_group.workernode.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = -1
  to_port           = -1
  ip_protocol       = "icmp"
  description       = "Allow ICMP from anywhere"
}

resource "aws_vpc_security_group_ingress_rule" "cp_icmp" {
  security_group_id = aws_security_group.controlplane.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = -1
  to_port           = -1
  ip_protocol       = "icmp"
  description       = "Allow ICMP from anywhere"
}

# egress

resource "aws_vpc_security_group_egress_rule" "bh_all" {
  security_group_id = aws_security_group.bastionhost.id
  cidr_ipv4         = "0.0.0.0/0"
  ip_protocol       = "-1"
  description       = "allow all outbound traffic"
}
resource "aws_vpc_security_group_egress_rule" "wn_all" {
  security_group_id = aws_security_group.workernode.id
  cidr_ipv4         = "0.0.0.0/0"
  ip_protocol       = "-1"
  description       = "allow all outbound traffic"
}

resource "aws_vpc_security_group_egress_rule" "cp_all" {
  security_group_id = aws_security_group.controlplane.id
  cidr_ipv4         = "0.0.0.0/0"
  ip_protocol       = "-1"
  description       = "allow all outbound traffic"
}